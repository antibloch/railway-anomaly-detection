{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04945e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import presets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import utils\n",
    "from torch import nn\n",
    "import json\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from dataset import FishyrailsCroppedDataset\n",
    "from autoencoder_networks import AeSegParam02\n",
    "from torchgeometry.losses.ssim import SSIM\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from patchclass_networks import PatchClassModel, PatchSegModelLight\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3d8be7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_students(teacher_model, student1_model, student2_model, student3_model, data_loader, g_act, obstacle_threshold, patch_size, device, idx_list, vis_path=None, mean=None, std=None, overall_max=1):\n",
    "    teacher_model.eval()\n",
    "    student1_model.eval()\n",
    "    student2_model.eval()\n",
    "    student3_model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "\n",
    "    if vis_path:\n",
    "        utils.mkdir(vis_path)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    storage = list()\n",
    "\n",
    "    # Compute max if necessary:\n",
    "    print(f\"Overall Max: {overall_max}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (image_fishy, target_fishy, image_orig, target_orig) in enumerate(metric_logger.log_every(data_loader, 100, header)):\n",
    "            \n",
    "            if idx not in idx_list:\n",
    "                continue\n",
    "            \n",
    "            print(f\"Image {idx} ...\")\n",
    "            idx_results = dict()\n",
    "                \n",
    "            # Go over fishy and orig:\n",
    "            for mode in [\"fishy\", \"orig\"]:\n",
    "                # Prepare everything\n",
    "                if mode == \"fishy\":\n",
    "                    image, target_seg = image_fishy.to(device), target_fishy.to(device)\n",
    "                elif mode == \"orig\":\n",
    "                    image, target_seg = image_orig.to(device), target_orig.to(device)\n",
    "                target_seg_orig = target_orig.clone().to(device)\n",
    "\n",
    "                # Mask for evaluation (discard background)\n",
    "                evaluation_mask = target_seg_orig == 1\n",
    "                target_seg[torch.logical_not(evaluation_mask)] = 0\n",
    "                evaluation_mask = evaluation_mask.squeeze()\n",
    "\n",
    "                # Visualize original image\n",
    "                if g_act == \"tanh\":\n",
    "                    image_target_ae, _ = presets.denormalize_tanh(image, image)  # (-1, 1)\n",
    "                    image_vis_pil, _ = presets.re_convert_tanh(image_target_ae, image_target_ae)\n",
    "                else:\n",
    "                    image_target_ae, _ = presets.denormalize(image, image)  # (0, 1)\n",
    "                    image_vis_pil, _ = presets.re_convert(image_target_ae, image_target_ae)\n",
    "                image_vis = np.asarray(image_vis_pil)\n",
    "                idx_results[f\"image_{mode}\"] = image_vis\n",
    "\n",
    "                image_ae_vis = np.ones_like(image_vis)*255 # dummy\n",
    "                idx_results[f\"ae_{mode}\"] = image_ae_vis\n",
    "        \n",
    "\n",
    "                # Prepare input for PatchSeg model\n",
    "                input_seg = image\n",
    "\n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    outputs_teacher = teacher(input_seg)[\"descriptor\"]\n",
    "                    normalized_teacher = F.normalize(outputs_teacher, mean=mean[\"teacher\"], std=std[\"teacher\"]).clone().detach()\n",
    "                    outputs_student1 = student1(input_seg)[\"descriptor\"]\n",
    "                    outputs_student2 = student2(input_seg)[\"descriptor\"]\n",
    "                    outputs_student3 = student3(input_seg)[\"descriptor\"]\n",
    "                    output_e_students = 1/3 * (outputs_student1 + outputs_student2 + outputs_student3)\n",
    "                    output_e = torch.squeeze(torch.square(normalized_teacher - output_e_students))\n",
    "                    output_e = torch.sum(output_e, dim=0)\n",
    "                    output_e_normalized = torch.abs((output_e - mean[\"e\"]) / std[\"e\"])\n",
    "                    output_v_mean = torch.sum(torch.squeeze(torch.square(output_e_students)), dim=0)\n",
    "                    output_v_student1 = torch.sum(torch.squeeze(torch.square(outputs_student1)), dim=0)\n",
    "                    output_v_student2 = torch.sum(torch.squeeze(torch.square(outputs_student2)), dim=0)\n",
    "                    output_v_student3 = torch.sum(torch.squeeze(torch.square(outputs_student3)), dim=0)\n",
    "                    output_v = 1/3 * (output_v_student1 + output_v_student2 + output_v_student3) - output_v_mean\n",
    "                    output_v_normalized = torch.abs((output_v - mean[\"v\"]) / std[\"v\"])\n",
    "                    output_seg = output_e_normalized + output_v_normalized\n",
    "\n",
    "                # Make sure segmentation outputs are in range (0, 1):\n",
    "                output_seg[output_seg > overall_max] = overall_max\n",
    "                output_seg = output_seg / overall_max\n",
    "                # Check if segmentation outputs are in range (0, 1):\n",
    "                if torch.max(output_seg) > 1 or torch.min(output_seg) < 0:\n",
    "                    print(\"ERROR: Output segmentation out of range!\")\n",
    "                    return\n",
    "\n",
    "                # Compute whether an obstacle can be found in seg output based on patch density:\n",
    "                kernel = torch.tensor(np.ones((patch_size, patch_size))* 1/(patch_size*patch_size)).view(1, 1, patch_size, patch_size).type(torch.FloatTensor)\n",
    "                output_seg_masked = output_seg.clone()\n",
    "                output_seg_masked[torch.logical_not(evaluation_mask)] = 0\n",
    "                patch_density = torch.nn.functional.conv2d(output_seg_masked.unsqueeze(0).unsqueeze(1), kernel, padding='same')\n",
    "                max_patch_density = torch.max(patch_density)\n",
    "                patch_density[patch_density <= obstacle_threshold] = 0\n",
    "                patch_density.squeeze()\n",
    "                if max_patch_density > obstacle_threshold:\n",
    "                    found_obstacle = 1\n",
    "                    # Compute centroid based on patch density\n",
    "                    x = torch.linspace(0, 223, steps=224).unsqueeze(0)\n",
    "                    x = x.repeat(224, 1)\n",
    "                    y = torch.linspace(0, 223, steps=224).unsqueeze(1)\n",
    "                    y = y.repeat(1, 224)\n",
    "                    centroid_x = int(\n",
    "                        torch.floor(torch.sum(patch_density * x) / torch.sum(patch_density)).type(torch.LongTensor))\n",
    "                    centroid_y = int(\n",
    "                        torch.floor(torch.sum(patch_density * y) / torch.sum(patch_density)).type(torch.LongTensor))\n",
    "                else:\n",
    "                    found_obstacle = 0\n",
    "\n",
    "\n",
    "                # Compute whether an obstacle can be found in groundtruth:\n",
    "                target_seg_masked = target_seg.clone().type(torch.FloatTensor)\n",
    "                target_seg_masked[target_seg == 1] = 0\n",
    "                target_seg_masked[target_seg == 2] = 1\n",
    "                patch_density_target = torch.nn.functional.conv2d(target_seg_masked, kernel, padding='same')\n",
    "                max_patch_density_target = torch.max(patch_density_target)\n",
    "                if max_patch_density_target > 0.3:\n",
    "                    has_obstacle = 1\n",
    "                    # Get bounding box\n",
    "                    target_seg_masked_pil = presets.torch_mask_to_pil(target_seg_masked)\n",
    "                    l_bb, u_bb, r_bb, d_bb = target_seg_masked_pil.getbbox()\n",
    "                else:\n",
    "                    has_obstacle = 0\n",
    "\n",
    "                # Check whether found obstacle was correct\n",
    "                if found_obstacle == 1 and has_obstacle == 1 and l_bb < centroid_x < r_bb and u_bb < centroid_y < d_bb:\n",
    "                    found_correct = True\n",
    "                else:\n",
    "                    found_correct = False\n",
    "\n",
    "                # Visualize Patch Density\n",
    "                patch_density[patch_density > 0] = 1\n",
    "                patch_density_vis_gray = patch_density * 255\n",
    "                patch_density_vis_gray = presets.torch_mask_to_pil(patch_density_vis_gray)\n",
    "                patch_density_vis = Image.new(\"RGB\", patch_density_vis_gray.size)\n",
    "                patch_density_vis.paste(patch_density_vis_gray)\n",
    "                patch_density_vis = Image.blend(patch_density_vis, image_vis_pil, 0.5)\n",
    "                draw = ImageDraw.Draw(patch_density_vis)\n",
    "                if found_obstacle == 1:\n",
    "                    if found_correct == 1:\n",
    "                        draw.ellipse((centroid_x-5 , centroid_y-5, centroid_x+5 , centroid_y+5), fill=\"green\")\n",
    "                    else:\n",
    "                        draw.ellipse((centroid_x - 5, centroid_y - 5, centroid_x + 5, centroid_y + 5), fill=\"red\")\n",
    "                if has_obstacle == 1:\n",
    "                    if found_correct == 1:\n",
    "                        draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"green\", width=5)\n",
    "                    else:\n",
    "                        draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"red\", width=5)\n",
    "                patch_density_vis = np.asarray(patch_density_vis)\n",
    "                idx_results[f\"pred_detect_{mode}\"] = patch_density_vis\n",
    "                \n",
    "                # Visualize Target Patch Density\n",
    "                patch_density_target[patch_density_target < 0.3] = 0\n",
    "                patch_density_target[patch_density_target > 0] = 1\n",
    "                patch_density_target = patch_density_target.squeeze()\n",
    "                patch_density_target_vis_gray = patch_density_target * 255\n",
    "                patch_density_target_vis_gray = presets.torch_mask_to_pil(patch_density_target_vis_gray)\n",
    "                patch_density_target_vis = Image.new(\"RGB\", patch_density_target_vis_gray.size)\n",
    "                patch_density_target_vis.paste(patch_density_target_vis_gray)\n",
    "                patch_density_target_vis = Image.blend(patch_density_target_vis, image_vis_pil, 0.5)\n",
    "                draw = ImageDraw.Draw(patch_density_target_vis)\n",
    "                if has_obstacle == 1:\n",
    "                    draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"blue\", width=5)\n",
    "                patch_density_target_vis = np.asarray(patch_density_target_vis)\n",
    "                idx_results[f\"target_detect_{mode}\"] = patch_density_target_vis\n",
    "\n",
    "                # Visualized output seg masked\n",
    "                output_seg_masked_vis_gray = output_seg\n",
    "                output_seg_masked_vis_gray[torch.logical_not(evaluation_mask)] = 0.5\n",
    "                output_seg_masked_vis_gray = output_seg_masked_vis_gray * 255\n",
    "                output_seg_masked_vis_gray = presets.torch_mask_to_pil(output_seg_masked_vis_gray)\n",
    "                output_seg_masked_vis = Image.new(\"RGB\", output_seg_masked_vis_gray.size)\n",
    "                output_seg_masked_vis.paste(output_seg_masked_vis_gray)\n",
    "                draw = ImageDraw.Draw(output_seg_masked_vis)\n",
    "                output_seg_masked_vis = np.asarray(output_seg_masked_vis)\n",
    "                idx_results[f\"pred_seg_{mode}\"] = output_seg_masked_vis\n",
    "\n",
    "                # Visualize target segmentation\n",
    "                target_obs_seg_vis_gray = torch.zeros_like(target_seg)\n",
    "                target_obs_seg_vis_gray[target_seg == 2] = 255 # obstacle white\n",
    "                target_obs_seg_vis_gray[target_seg == 0] = 127 # background gray\n",
    "                target_obs_seg_vis_gray = presets.torch_mask_to_pil(target_obs_seg_vis_gray)\n",
    "                target_obs_seg_vis = Image.new(\"RGB\", target_obs_seg_vis_gray.size)\n",
    "                target_obs_seg_vis.paste(target_obs_seg_vis_gray)\n",
    "                draw = ImageDraw.Draw(target_obs_seg_vis)\n",
    "                target_obs_seg_vis = np.asarray(target_obs_seg_vis)\n",
    "                idx_results[f\"target_seg_{mode}\"] = target_obs_seg_vis\n",
    "            storage.append(idx_results)\n",
    "\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0821c2e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(ae_model, ae_model_name, model, model_name, data_loader, g_act, obstacle_threshold, patch_size, device, idx_list, vis_path=None, mean=None, std=None):\n",
    "    if ae_model:\n",
    "        ae_model.eval()\n",
    "    if model:\n",
    "        model.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "\n",
    "    if vis_path:\n",
    "        utils.mkdir(vis_path)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    storage = list()\n",
    "\n",
    "    # Compute max if necessary:\n",
    "    if model_name == \"mse\":\n",
    "        overall_max = 0.986\n",
    "    elif model_name == \"patchsegmodellight\" and ae_model_name == \"patchsegmodellight\":  # Student Teacher\n",
    "        overall_max = 10.796\n",
    "    else:\n",
    "        overall_max = 1.0\n",
    "    print(f\"Overall Max: {overall_max}\")\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (image_fishy, target_fishy, image_orig, target_orig) in enumerate(metric_logger.log_every(data_loader, 100, header)):\n",
    "            \n",
    "            if idx not in idx_list:\n",
    "                continue\n",
    "            \n",
    "            print(f\"Image {idx} ...\")\n",
    "            idx_results = dict()\n",
    "                \n",
    "            # Go over fishy and orig:\n",
    "            for mode in [\"fishy\", \"orig\"]:\n",
    "                # Prepare everything\n",
    "                if mode == \"fishy\":\n",
    "                    image, target_seg = image_fishy.to(device), target_fishy.to(device)\n",
    "                elif mode == \"orig\":\n",
    "                    image, target_seg = image_orig.to(device), target_orig.to(device)\n",
    "                target_seg_orig = target_orig.clone().to(device)\n",
    "\n",
    "                # Mask for evaluation (discard background)\n",
    "                evaluation_mask = target_seg_orig == 1\n",
    "                target_seg[torch.logical_not(evaluation_mask)] = 0\n",
    "                evaluation_mask = evaluation_mask.squeeze()\n",
    "\n",
    "                # Visualize original image\n",
    "                if g_act == \"tanh\":\n",
    "                    image_target_ae, _ = presets.denormalize_tanh(image, image)  # (-1, 1)\n",
    "                    image_vis_pil, _ = presets.re_convert_tanh(image_target_ae, image_target_ae)\n",
    "                else:\n",
    "                    image_target_ae, _ = presets.denormalize(image, image)  # (0, 1)\n",
    "                    image_vis_pil, _ = presets.re_convert(image_target_ae, image_target_ae)\n",
    "                image_vis = np.asarray(image_vis_pil)\n",
    "                idx_results[f\"image_{mode}\"] = image_vis\n",
    "\n",
    "                if ae_model and ae_model_name != \"patchsegmodellight\":\n",
    "                    # Run  AE inference\n",
    "                    with torch.no_grad():\n",
    "                        outputs = ae_model(image)\n",
    "                    output_ae = outputs[\"out_aa\"]\n",
    "\n",
    "                    # Post-process AE image for PatchSeg\n",
    "                    if g_act == \"tanh\":\n",
    "                        image_ae = (output_ae / 2) + 0.5\n",
    "                    else:\n",
    "                        image_ae = output_ae\n",
    "                    image_ae = torchvision.transforms.functional.normalize(image_ae, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "                    # Visualize AE image\n",
    "                    if g_act == \"tanh\":\n",
    "                        image_ae_vis, _ = presets.re_convert_tanh(output_ae, output_ae)  # no de-normalization\n",
    "                    else:\n",
    "                        image_ae_vis, _ = presets.re_convert(output_ae, output_ae)  # no de-normalization\n",
    "                    \n",
    "                    image_ae_vis = np.asarray(image_ae_vis)\n",
    "                else:\n",
    "                    image_ae_vis = np.ones_like(image_vis)*255 # dummy\n",
    "                idx_results[f\"ae_{mode}\"] = image_ae_vis\n",
    "        \n",
    "\n",
    "                # Prepare input for PatchSeg model\n",
    "                if \"patch30\" in model_name:\n",
    "                    image_r1 = torch.zeros_like(image)\n",
    "                    image_r1[::, :-27] = image[::, 27:]\n",
    "                    image_r2 = torch.zeros_like(image)\n",
    "                    image_r2[::, :-54] = image[::, 54:]\n",
    "                    image_l1 = torch.zeros_like(image)\n",
    "                    image_l1[::, 27:] = image[::, :-27]\n",
    "                    image_l2 = torch.zeros_like(image)\n",
    "                    image_l2[::, 54:] = image[::, :-54]\n",
    "                    image_ae_r1 = torch.zeros_like(image_ae)\n",
    "                    image_ae_r1[::, :-27] = image_ae[::, 27:]\n",
    "                    image_ae_r2 = torch.zeros_like(image_ae)\n",
    "                    image_ae_r2[::, :-54] = image_ae[::, 54:]\n",
    "                    image_ae_l1 = torch.zeros_like(image_ae)\n",
    "                    image_ae_l1[::, 27:] = image_ae[::, :-27]\n",
    "                    image_ae_l2 = torch.zeros_like(image_ae)\n",
    "                    image_ae_l2[::, 54:] = image_ae[::, :-54]\n",
    "                    input_seg = torch.cat((image_l2, image_l1, image, image_r1, image_r2, image_ae_l2, image_ae_l1, image_ae, image_ae_r1, image_ae_r2), dim=1)\n",
    "                elif \"patch15\" in model_name:\n",
    "                    image_r1 = torch.zeros_like(image)\n",
    "                    image_r1[::, :-27] = image[::, 27:]\n",
    "                    image_r2 = torch.zeros_like(image)\n",
    "                    image_r2[::, :-54] = image[::, 54:]\n",
    "                    image_l1 = torch.zeros_like(image)\n",
    "                    image_l1[::, 27:] = image[::, :-27]\n",
    "                    image_l2 = torch.zeros_like(image)\n",
    "                    image_l2[::, 54:] = image[::, :-54]\n",
    "                    input_seg = torch.cat((image_l2, image_l1, image, image_r1, image_r2), dim=1)\n",
    "                elif \"patch6\" in model_name:\n",
    "                    input_seg = torch.cat((image, image_ae), dim=1)\n",
    "                else:\n",
    "                    input_seg = image\n",
    "\n",
    "                # Inference\n",
    "                if \"patchsegmodel03\" in model_name:\n",
    "                    with torch.no_grad():\n",
    "                        output_seg = model(input_seg)[\"out\"]\n",
    "                        output_seg = nn.functional.softmax(output_seg, dim=1)\n",
    "                        output_seg = output_seg[0, 0, ::]\n",
    "                elif model_name == \"deeplabv3_resnet50\":\n",
    "                    with torch.no_grad():\n",
    "                        output_seg = model(input_seg)[\"out\"]\n",
    "                        output_seg = nn.functional.softmax(output_seg, dim=1)\n",
    "                        output_seg = output_seg[0, 0, ::]\n",
    "                elif model_name == \"mse\":\n",
    "                    if g_act == \"tanh\":\n",
    "                        image_target_ae = (image_target_ae / 2) + 0.5\n",
    "                        output_ae = (output_ae / 2) + 0.5\n",
    "                    output_seg = torch.squeeze(torch.sqrt(torch.square(image_target_ae - output_ae)))\n",
    "                    output_seg = torch.mean(output_seg, dim=0)\n",
    "                elif model_name == \"ssim\":\n",
    "                    ssim = SSIM(11)\n",
    "                    if g_act == \"tanh\":\n",
    "                        image_target_ae = (image_target_ae / 2) + 0.5\n",
    "                        output_ae = (output_ae / 2) + 0.5\n",
    "                    output_seg = torch.squeeze(ssim(image_target_ae, output_ae))*2 # SSIM output is in range (0, 0.5)\n",
    "                    output_seg = torch.mean(output_seg, dim=0)\n",
    "                elif model_name == \"patchsegmodellight\" and model_name == \"patchsegmodellight\": # Student Teacher\n",
    "                    with torch.no_grad():\n",
    "                        outputs_teacher = ae_model(input_seg)[\"descriptor\"]\n",
    "                        normalized_teacher = F.normalize(outputs_teacher, mean=mean, std=std).clone().detach()\n",
    "                        outputs_student = model(input_seg)[\"descriptor\"]\n",
    "                        output_seg = torch.squeeze(torch.sqrt(torch.square(normalized_teacher - outputs_student)))\n",
    "                        # print(f\"Output_seg: {output_seg.shape}, max: {torch.max(output_seg)}, min: {torch.min(output_seg)}\")\n",
    "                        # Make sure output_seg is in range (0, 1)\n",
    "                        output_seg = torch.mean(output_seg, dim=0)\n",
    "\n",
    "                # Make sure segmentation outputs are in range (0, 1):\n",
    "                output_seg = output_seg / overall_max\n",
    "                # Check if segmentation outputs are in range (0, 1):\n",
    "                if torch.max(output_seg) > 1 or torch.min(output_seg) < 0:\n",
    "                    print(\"ERROR: Output segmentation out of range!\")\n",
    "                    return\n",
    "\n",
    "                # Compute whether an obstacle can be found in seg output based on patch density:\n",
    "                kernel = torch.tensor(np.ones((patch_size, patch_size))* 1/(patch_size*patch_size)).view(1, 1, patch_size, patch_size).type(torch.FloatTensor)\n",
    "                output_seg_masked = output_seg.clone()\n",
    "                output_seg_masked[torch.logical_not(evaluation_mask)] = 0\n",
    "                patch_density = torch.nn.functional.conv2d(output_seg_masked.unsqueeze(0).unsqueeze(1), kernel, padding='same')\n",
    "                max_patch_density = torch.max(patch_density)\n",
    "                patch_density[patch_density <= obstacle_threshold] = 0\n",
    "                patch_density.squeeze()\n",
    "                if max_patch_density > obstacle_threshold:\n",
    "                    found_obstacle = 1\n",
    "                    # Compute centroid based on patch density\n",
    "                    x = torch.linspace(0, 223, steps=224).unsqueeze(0)\n",
    "                    x = x.repeat(224, 1)\n",
    "                    y = torch.linspace(0, 223, steps=224).unsqueeze(1)\n",
    "                    y = y.repeat(1, 224)\n",
    "                    centroid_x = int(\n",
    "                        torch.floor(torch.sum(patch_density * x) / torch.sum(patch_density)).type(torch.LongTensor))\n",
    "                    centroid_y = int(\n",
    "                        torch.floor(torch.sum(patch_density * y) / torch.sum(patch_density)).type(torch.LongTensor))\n",
    "                else:\n",
    "                    found_obstacle = 0\n",
    "\n",
    "\n",
    "                # Compute whether an obstacle can be found in groundtruth:\n",
    "                target_seg_masked = target_seg.clone().type(torch.FloatTensor)\n",
    "                target_seg_masked[target_seg == 1] = 0\n",
    "                target_seg_masked[target_seg == 2] = 1\n",
    "                patch_density_target = torch.nn.functional.conv2d(target_seg_masked, kernel, padding='same')\n",
    "                max_patch_density_target = torch.max(patch_density_target)\n",
    "                if max_patch_density_target > 0.3:\n",
    "                    has_obstacle = 1\n",
    "                    # Get bounding box\n",
    "                    target_seg_masked_pil = presets.torch_mask_to_pil(target_seg_masked)\n",
    "                    l_bb, u_bb, r_bb, d_bb = target_seg_masked_pil.getbbox()\n",
    "                else:\n",
    "                    has_obstacle = 0\n",
    "\n",
    "                # Check whether found obstacle was correct\n",
    "                if found_obstacle == 1 and has_obstacle == 1 and l_bb < centroid_x < r_bb and u_bb < centroid_y < d_bb:\n",
    "                    found_correct = True\n",
    "                else:\n",
    "                    found_correct = False\n",
    "\n",
    "                # Visualize Patch Density\n",
    "                patch_density[patch_density > 0] = 1\n",
    "                patch_density_vis_gray = patch_density * 255\n",
    "                patch_density_vis_gray = presets.torch_mask_to_pil(patch_density_vis_gray)\n",
    "                patch_density_vis = Image.new(\"RGB\", patch_density_vis_gray.size)\n",
    "                patch_density_vis.paste(patch_density_vis_gray)\n",
    "                patch_density_vis = Image.blend(patch_density_vis, image_vis_pil, 0.5)\n",
    "                draw = ImageDraw.Draw(patch_density_vis)\n",
    "                if found_obstacle == 1:\n",
    "                    if found_correct == 1:\n",
    "                        draw.ellipse((centroid_x-5 , centroid_y-5, centroid_x+5 , centroid_y+5), fill=\"green\")\n",
    "                    else:\n",
    "                        draw.ellipse((centroid_x - 5, centroid_y - 5, centroid_x + 5, centroid_y + 5), fill=\"red\")\n",
    "                if has_obstacle == 1:\n",
    "                    if found_correct == 1:\n",
    "                        draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"green\", width=5)\n",
    "                    else:\n",
    "                        draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"red\", width=5)\n",
    "                patch_density_vis = np.asarray(patch_density_vis)\n",
    "                idx_results[f\"pred_detect_{mode}\"] = patch_density_vis\n",
    "                \n",
    "                # Visualize Target Patch Density\n",
    "                patch_density_target[patch_density_target < 0.3] = 0\n",
    "                patch_density_target[patch_density_target > 0] = 1\n",
    "                patch_density_target = patch_density_target.squeeze()\n",
    "                patch_density_target_vis_gray = patch_density_target * 255\n",
    "                patch_density_target_vis_gray = presets.torch_mask_to_pil(patch_density_target_vis_gray)\n",
    "                patch_density_target_vis = Image.new(\"RGB\", patch_density_target_vis_gray.size)\n",
    "                patch_density_target_vis.paste(patch_density_target_vis_gray)\n",
    "                patch_density_target_vis = Image.blend(patch_density_target_vis, image_vis_pil, 0.5)\n",
    "                draw = ImageDraw.Draw(patch_density_target_vis)\n",
    "                if has_obstacle == 1:\n",
    "                    draw.rectangle((l_bb, u_bb, r_bb, d_bb), outline=\"blue\", width=5)\n",
    "                patch_density_target_vis = np.asarray(patch_density_target_vis)\n",
    "                idx_results[f\"target_detect_{mode}\"] = patch_density_target_vis\n",
    "\n",
    "                # Visualized output seg masked\n",
    "                output_seg_masked_vis_gray = output_seg\n",
    "                output_seg_masked_vis_gray[torch.logical_not(evaluation_mask)] = 0.5\n",
    "                output_seg_masked_vis_gray = output_seg_masked_vis_gray * 255\n",
    "                output_seg_masked_vis_gray = presets.torch_mask_to_pil(output_seg_masked_vis_gray)\n",
    "                output_seg_masked_vis = Image.new(\"RGB\", output_seg_masked_vis_gray.size)\n",
    "                output_seg_masked_vis.paste(output_seg_masked_vis_gray)\n",
    "                draw = ImageDraw.Draw(output_seg_masked_vis)\n",
    "                output_seg_masked_vis = np.asarray(output_seg_masked_vis)\n",
    "                idx_results[f\"pred_seg_{mode}\"] = output_seg_masked_vis\n",
    "\n",
    "                # Visualize target segmentation\n",
    "                target_obs_seg_vis_gray = torch.zeros_like(target_seg)\n",
    "                target_obs_seg_vis_gray[target_seg == 2] = 255 # obstacle white\n",
    "                target_obs_seg_vis_gray[target_seg == 0] = 127 # background gray\n",
    "                target_obs_seg_vis_gray = presets.torch_mask_to_pil(target_obs_seg_vis_gray)\n",
    "                target_obs_seg_vis = Image.new(\"RGB\", target_obs_seg_vis_gray.size)\n",
    "                target_obs_seg_vis.paste(target_obs_seg_vis_gray)\n",
    "                draw = ImageDraw.Draw(target_obs_seg_vis)\n",
    "                target_obs_seg_vis = np.asarray(target_obs_seg_vis)\n",
    "                idx_results[f\"target_seg_{mode}\"] = target_obs_seg_vis\n",
    "            storage.append(idx_results)\n",
    "\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe13fb7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model_name, checkpoint_name, ae_model_name, ae_checkpoint_name, stages, g_act, device):\n",
    "\n",
    "    print(f\"Running on device: {device}\")\n",
    "\n",
    "    # Gan model\n",
    "    if ae_model_name == \"AeSegParam02_8810\":\n",
    "        ae_model = AeSegParam02(c_seg=8, c_ae=8, c_param=1, mode=\"none\", ratio=0, act=g_act)\n",
    "    elif ae_model_name == \"AeSegParam02_8410\":\n",
    "        ae_model = AeSegParam02(c_seg=8, c_ae=4, c_param=1, mode=\"none\", ratio=0, act=g_act)\n",
    "    elif ae_model_name == \"AeSegParam02_8210\":\n",
    "        ae_model = AeSegParam02(c_seg=8, c_ae=2, c_param=1, mode=\"none\", ratio=0, act=g_act)\n",
    "    elif ae_model_name == \"AeSegParam02_8110\":\n",
    "        ae_model = AeSegParam02(c_seg=8, c_ae=1, c_param=1, mode=\"none\", ratio=0, act=g_act)\n",
    "    elif ae_model_name == \"patchsegmodellight\":\n",
    "        ae_model = PatchSegModelLight(in_channels=3, out_channels=512, stages=stages, patch_only=False)\n",
    "    else:\n",
    "        ae_model = None\n",
    "        print(f\"No autoencoder used!\")\n",
    "\n",
    "    if ae_model:\n",
    "        ae_model.to(device)\n",
    "        ae_checkpoint = torch.load(ae_checkpoint_name, map_location=\"cpu\")\n",
    "        ae_model.load_state_dict(ae_checkpoint[\"model\"], strict=False)\n",
    "        print(\"AE Model loaded.\")\n",
    "        if ae_model_name == \"patchsegmodellight\" and model_name == \"patchsegmodellight\":\n",
    "            mean_std_dir = os.path.dirname(ae_checkpoint_name)\n",
    "            mean_std_suffix = os.path.basename(ae_checkpoint_name[:-4])\n",
    "            with open(os.path.join(mean_std_dir, f\"{mean_std_suffix}_mean.npy\"), \"rb\") as file:\n",
    "                mean = np.load(file)\n",
    "            with open(os.path.join(mean_std_dir, f\"{mean_std_suffix}_std.npy\"), \"rb\") as file:\n",
    "                std = np.load(file)\n",
    "            mean = torch.from_numpy(mean).to(device)\n",
    "            print(f\"Mean shape: {mean.shape}\")\n",
    "            std = torch.from_numpy(std).to(device)\n",
    "            print(f\"Std shape: {std.shape}\")\n",
    "        else:\n",
    "            mean = None\n",
    "            std = None\n",
    "    else:\n",
    "        mean = None\n",
    "        std = None\n",
    "\n",
    "    # Segmentation model\n",
    "    if model_name == \"deeplabv3_resnet50\":\n",
    "        model = torchvision.models.segmentation.__dict__[model_name](\n",
    "                pretrained=False,\n",
    "                pretrained_backbone=False,\n",
    "                num_classes=2,\n",
    "                aux_loss=False,\n",
    "            )\n",
    "        #model.backbone.conv1 = nn.Conv2d(6, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "    elif model_name == \"patchsegmodellight_patch30\":\n",
    "        model = PatchSegModelLight(stages=stages, in_channels=30)\n",
    "    elif model_name == \"patchsegmodel03_patch30\":\n",
    "        model = PatchClassModel(stages=stages, in_channels=30)\n",
    "    elif model_name == \"patchsegmodel03_patch15\":\n",
    "        model = PatchClassModel(stages=stages, in_channels=15)\n",
    "    elif model_name == \"patchsegmodel03_patch6\":\n",
    "        model = PatchClassModel(stages=stages, in_channels=6)\n",
    "    elif model_name == \"patchsegmodel03_patch3\":\n",
    "        model = PatchClassModel(stages=stages, in_channels=3)\n",
    "    elif model_name == \"patchsegmodellight\":\n",
    "        model = PatchSegModelLight(in_channels=3, out_channels=512, stages=stages, patch_only=False)\n",
    "    else:\n",
    "        model = None\n",
    "        print(\"No seg model!\")\n",
    "\n",
    "    if model:\n",
    "        model.to(device)\n",
    "        checkpoint = torch.load(checkpoint_name, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "    return model, ae_model, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f38335",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_students(teacher_checkpoint_name, student1_checkpoint_name, student2_checkpoint_name, student3_checkpoint_name, stages, device):\n",
    "\n",
    "    print(f\"Running on device: {device}\")\n",
    "\n",
    "    # Models\n",
    "    teacher = PatchSegModelLight(in_channels=3, out_channels=128, stages=stages, patch_only=False).to(device)\n",
    "    student1 = PatchSegModelLight(in_channels=3, out_channels=128, stages=stages, patch_only=False).to(device)\n",
    "    student2 = PatchSegModelLight(in_channels=3, out_channels=128, stages=stages, patch_only=False).to(device)\n",
    "    student3 = PatchSegModelLight(in_channels=3, out_channels=128, stages=stages, patch_only=False).to(device)\n",
    "\n",
    "    teacher_checkpoint = torch.load(teacher_checkpoint_name, map_location=\"cpu\")\n",
    "    teacher.load_state_dict(teacher_checkpoint[\"model\"], strict=False)\n",
    "    student1_checkpoint = torch.load(student1_checkpoint_name, map_location=\"cpu\")\n",
    "    student1.load_state_dict(student1_checkpoint[\"model\"], strict=False)\n",
    "    student2_checkpoint = torch.load(student2_checkpoint_name, map_location=\"cpu\")\n",
    "    student2.load_state_dict(student2_checkpoint[\"model\"], strict=False)\n",
    "    student3_checkpoint = torch.load(student3_checkpoint_name, map_location=\"cpu\")\n",
    "    student3.load_state_dict(student3_checkpoint[\"model\"], strict=False)\n",
    "\n",
    "    # Load Mean and Std (pre-computed)\n",
    "    mean_std_dir = os.path.dirname(teacher_checkpoint_name)\n",
    "    mean_std_suffix = os.path.basename(teacher_checkpoint_name[:-4])\n",
    "    with open(os.path.join(mean_std_dir, f\"{mean_std_suffix}_mean.npy\"), \"rb\") as file:\n",
    "        mean = np.load(file)\n",
    "    with open(os.path.join(mean_std_dir, f\"{mean_std_suffix}_std.npy\"), \"rb\") as file:\n",
    "        std = np.load(file)\n",
    "    mean_teacher = torch.from_numpy(mean).to(device)\n",
    "    print(f\"Mean shape: {mean.shape}\")\n",
    "    std_teacher = torch.from_numpy(std).to(device)\n",
    "    print(f\"Std shape: {std.shape}\")\n",
    "\n",
    "    return teacher, student1, student2, student3, mean_teacher, std_teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d86a5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_list_final_good_orig = [14, 44, 156, 162, 184, 280] # 200, 447\n",
    "idx_list_final_good_fishy = [23, 107, 204, 321, 733, 3173] # 58, 41\n",
    "idx_list_final_bad_orig_1 = [133, 1426]\n",
    "idx_list_final_bad_orig_2 = [293, 1967]\n",
    "idx_list_final_bad_fishy_1 = [63, 998]\n",
    "idx_list_final_bad_fishy_2 = [66, 75]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Results \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# Define args\n",
    "output_path = \"./evaluations\"\n",
    "data_path = \"./datasets/FishyrailsCroppedv1.h5\"\n",
    "\n",
    "idx_list = idx_list_final_good_fishy\n",
    "idx_list = list(dict.fromkeys(idx_list)) # remove duplicates\n",
    "idx_list.sort()\n",
    "#idx_list = idx_list[:90]\n",
    "\n",
    "mymode = \"fishy\"\n",
    "method_list = []\n",
    "method_list.append(\n",
    "    {\"model_name\": \"deeplabv3_resnet50\", \n",
    "     \"checkpoint_name\": \"./trained_models/deeplabv3_model_5.pth\",\n",
    "     \"ae_model_name\": \"none\",\n",
    "     \"ae_checkpoint_name\": \"none\",\n",
    "     \"stages\": 99,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.3, \n",
    "     \"patch_size\": 51,\n",
    "     \"column1\": \"DeeplabV3\\n\",\n",
    "     \"column2\": \"-\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"mse\", \n",
    "     \"checkpoint_name\": \"none\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_mse_model_199.pth\",\n",
    "     \"stages\": 99,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.2, \n",
    "     \"patch_size\": 7,\n",
    "     \"column1\": \"RMSE AE\\n\"r\"$\\mathcal{L}_{MSE}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{MSE}$\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"ssim\", \n",
    "     \"checkpoint_name\": \"none\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_ssim_model_199.pth\",\n",
    "     \"stages\": 99,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.65,\n",
    "     \"patch_size\": 21,\n",
    "     \"column1\": \"SSIM AE\\n\"r\"$\\mathcal{L}_{SSIM}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{SSIM}$\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodellight\", \n",
    "     \"teacher_checkpoint_name\": \"./trained_models/teacher_33_model_49.pth\",\n",
    "     \"student1_checkpoint_name\": \"./trained_models/t33_student_1_model_40.pth\",\n",
    "     \"student2_checkpoint_name\": \"./trained_models/t33_student_2_model_40.pth\",\n",
    "     \"student3_checkpoint_name\": \"./trained_models/t33_student_3_model_40.pth\",\n",
    "     \"stages\": 2,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.2, \n",
    "     \"patch_size\": 35,\n",
    "     \"column1\": \"Students33\\n\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodel03_patch3\", \n",
    "     \"checkpoint_name\": \"./trained_models/patchclass_21_model_20.pth\",\n",
    "     \"ae_model_name\": \"none\",\n",
    "     \"ae_checkpoint_name\": \"none\",\n",
    "     \"stages\": 1,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.95, \n",
    "     \"patch_size\": 11,\n",
    "     \"column1\": \"PatchClass21\\n\",\n",
    "     \"column2\": \"-\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodel03_patch6\", \n",
    "     \"checkpoint_name\": \"./trained_models/patchdiff_21_mse_model_25.pth\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_mse_model_199.pth\",\n",
    "     \"stages\": 1,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.9, \n",
    "     \"patch_size\": 7,\n",
    "     \"column1\": \"PatchDiff21\\n\"r\"$\\mathcal{L}_{MSE}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{MSE}$\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodel03_patch6\", \n",
    "     \"checkpoint_name\": \"./trained_models/patchdiff_21_ssim_model_20.pth\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_ssim_model_199.pth\",\n",
    "     \"stages\": 1,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.95, \n",
    "     \"patch_size\": 7,\n",
    "     \"column1\": \"PatchDiff21\\n\"r\"$\\mathcal{L}_{SSIM}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{SSIM}$\",\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodel03_patch6\", \n",
    "     \"checkpoint_name\": \"./trained_models/patchdiff_21_gan_model_30.pth\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_gan_model_199.pth\",\n",
    "     \"stages\": 1,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.95,\n",
    "     \"patch_size\": 11,\n",
    "     \"column1\": \"PatchDiff21\\n\"r\"$\\mathcal{L}_{GAN}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{GAN}$\"\n",
    "    }\n",
    ")\n",
    "method_list.append(\n",
    "    {\"model_name\": \"patchsegmodel03_patch6\", \n",
    "     \"checkpoint_name\": \"./trained_models/patchdiff_21_gan+hist_model_20.pth\",\n",
    "     \"ae_model_name\": \"AeSegParam02_8810\",\n",
    "     \"ae_checkpoint_name\": \"./trained_models/ae_gan+hist_model_199.pth\",\n",
    "     \"stages\": 1,\n",
    "     \"g_act\": \"tanh\",\n",
    "     \"threshold\": 0.95,\n",
    "     \"patch_size\": 7,\n",
    "     \"column1\": \"PatchDiff21\\n\"r\"$\\mathcal{L}_{GAN} + \\mathcal{L}_{HIST}$\",\n",
    "     \"column2\": r\"$\\mathcal{L}_{GAN} + \\mathcal{L}_{HIST}$\",\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# Create Dataset\n",
    "dataset_test = FishyrailsCroppedDataset(data_path)\n",
    "print(\"Dataset loaded.\")\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, sampler=test_sampler, num_workers=1\n",
    ")\n",
    "print(\"Dataloader created.\")\n",
    "method_results = list()\n",
    "# Get models:\n",
    "for method in method_list:\n",
    "    model_name = method[\"model_name\"]\n",
    "    if model_name == \"patchsegmodellight\":\n",
    "        teacher_checkpoint_name = method[\"teacher_checkpoint_name\"]\n",
    "        student1_checkpoint_name = method[\"student1_checkpoint_name\"]\n",
    "        student2_checkpoint_name = method[\"student2_checkpoint_name\"]\n",
    "        student3_checkpoint_name = method[\"student3_checkpoint_name\"]\n",
    "        stages = method[\"stages\"]\n",
    "        g_act = method[\"g_act\"]\n",
    "        threshold = method[\"threshold\"]\n",
    "        patch_size = method[\"patch_size\"]\n",
    "        teacher, student1, student2, student3, mean_teacher, std_teacher = get_model_students(teacher_checkpoint_name, student1_checkpoint_name, student2_checkpoint_name, student3_checkpoint_name, stages, device)\n",
    "        mean = dict()\n",
    "        mean[\"teacher\"] = mean_teacher\n",
    "        mean[\"v\"] = 0.56520\n",
    "        mean[\"e\"] = 2.69930\n",
    "        std = dict()\n",
    "        std[\"teacher\"] = std_teacher\n",
    "        std[\"v\"] = 1.20560\n",
    "        std[\"e\"] = 5.07015\n",
    "        overall_max = 5 # 264 # 212\n",
    "        method_data = evaluate_students(teacher, student1, student2, student3, data_loader_test, g_act, threshold, patch_size, device, idx_list, vis_path=output_path, mean=mean, std=std, overall_max=overall_max)\n",
    "   \n",
    "    else:\n",
    "        print(f\"Method {method['column1']} ... \")\n",
    "        checkpoint_name = method[\"checkpoint_name\"]\n",
    "        ae_model_name = method[\"ae_model_name\"]\n",
    "        ae_checkpoint_name = method[\"ae_checkpoint_name\"]\n",
    "        stages = method[\"stages\"]\n",
    "        g_act = method[\"g_act\"]\n",
    "        threshold = method[\"threshold\"]\n",
    "        patch_size = method[\"patch_size\"]\n",
    "        model, ae_model, mean, std = get_model(model_name, checkpoint_name, ae_model_name, ae_checkpoint_name, stages, g_act, device)\n",
    "        method_data = evaluate(ae_model, ae_model_name, model, model_name, data_loader_test, g_act, threshold, patch_size, device, idx_list, vis_path=output_path, mean=mean, std=std)\n",
    "    method_results.append(method_data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualization Detection\n",
    "\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "#letters = ['G', 'H', 'I', 'J', 'K', 'L']\n",
    "\n",
    "num_rows = len(method_list) + 2\n",
    "num_cols = len(idx_list)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols,2*num_rows))\n",
    "plt.axis('off')\n",
    "\n",
    "# go over columns (images)\n",
    "for image_idx in range(len(idx_list)):\n",
    "    # Targets first\n",
    "    result_m = method_results[0][image_idx]\n",
    "    image_fishy = result_m[f\"image_{mymode}\"]\n",
    "    ax = axes[0, image_idx]\n",
    "    ax.imshow(image_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Image\\n\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.annotate(f\"{letters[image_idx]}\", xy=(0.5, 1), xytext=(0, 20),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    #ax.annotate(f\"{idx_list[image_idx]}\", size=14, xy=(0, 1), ha='left')\n",
    "    ax.axis('off')\n",
    "    target_seg_fishy = result_m[f\"target_detect_{mymode}\"]\n",
    "    ax = axes[1, image_idx]\n",
    "    ax.imshow(target_seg_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Target\\n\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.axis('off')\n",
    "    \n",
    "\n",
    "    # go over rows (methods)\n",
    "    for method_idx in range(len(method_results)):\n",
    "        result_m = method_results[method_idx][image_idx]\n",
    "        pred_seg_fishy = result_m[f\"pred_detect_{mymode}\"]\n",
    "        ax = axes[method_idx+2, image_idx]\n",
    "        ax.imshow(pred_seg_fishy)\n",
    "        if image_idx == 0:\n",
    "            ax.annotate(method_list[method_idx][\"column1\"], xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points', rotation=90,\n",
    "                size=14, ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.savefig(f\"./evaluations/results_final_det_{mymode}.pdf\", bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n",
    "# Visualization Difference Map\n",
    "\n",
    "num_rows = len(method_list) + 2\n",
    "num_cols = len(idx_list)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols,2*num_rows))\n",
    "plt.axis('off')\n",
    "\n",
    "# go over rows (images)\n",
    "for image_idx in range(len(idx_list)):\n",
    "    # Targets first\n",
    "    result_m = method_results[0][image_idx]\n",
    "    image_fishy = result_m[f\"image_{mymode}\"]\n",
    "    ax = axes[0, image_idx]\n",
    "    ax.imshow(image_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Image\\n\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.annotate(f\"{letters[image_idx]}\", xy=(0.5, 1), xytext=(0, 20),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    target_seg_fishy = result_m[f\"target_seg_{mymode}\"]\n",
    "    ax = axes[1, image_idx]\n",
    "    ax.imshow(target_seg_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Target\\n\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # go over rows (methods)\n",
    "    for method_idx in range(len(method_results)):\n",
    "        result_m = method_results[method_idx][image_idx]\n",
    "        pred_seg_fishy = result_m[f\"pred_seg_{mymode}\"]\n",
    "        ax = axes[method_idx+2, image_idx]\n",
    "        ax.imshow(pred_seg_fishy)\n",
    "        if image_idx == 0:\n",
    "            ax.annotate(method_list[method_idx][\"column1\"], xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points', rotation=90,\n",
    "                size=14, ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "plt.savefig(f\"./evaluations/results_final_seg_{mymode}.pdf\", bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n",
    "# Autoencoder Visualization\n",
    "\n",
    "method_list_subset = method_list[-4:]\n",
    "method_results_subset = method_results[-4:]\n",
    "\n",
    "num_rows = len(method_list_subset) + 1\n",
    "num_cols = len(idx_list)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols,2*num_rows))\n",
    "plt.axis('off')\n",
    "\n",
    "# go over rows (images)\n",
    "for image_idx in range(len(idx_list)):\n",
    "    # Targets first\n",
    "    result_m = method_results_subset[0][image_idx]\n",
    "    image_fishy = result_m[f\"image_{mymode}\"]\n",
    "    ax = axes[0, image_idx]\n",
    "    ax.imshow(image_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Input\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.annotate(f\"{letters[image_idx]}\", xy=(0.5, 1), xytext=(0, 20),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # go over columns (methods)\n",
    "    for method_idx in range(len(method_results_subset)):\n",
    "        result_m = method_results_subset[method_idx][image_idx]\n",
    "        ae_fishy = result_m[f\"ae_{mymode}\"]\n",
    "        ax = axes[method_idx+1, image_idx]\n",
    "        ax.imshow(ae_fishy)\n",
    "        if image_idx == 0:\n",
    "            ax.annotate(method_list_subset[method_idx][\"column2\"], xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points', rotation=90,\n",
    "                size=14, ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "        #ae_orig = result_m[\"ae_orig\"]\n",
    "\n",
    "plt.savefig(f\"./evaluations/results_final_ae_{mymode}.pdf\", bbox_inches='tight')\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bad\n",
    "\n",
    "letters = ['M', 'N']\n",
    "letters = ['O', 'P']\n",
    "letters = ['Q', 'R']\n",
    "#letters = ['S', 'T']\n",
    "\n",
    "method_list_subset = method_list[-5:]\n",
    "method_results_subset = method_results[-5:]\n",
    "\n",
    "num_rows = len(method_list_subset) + 1\n",
    "num_cols = 3*len(idx_list)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(2*num_cols,2*num_rows))\n",
    "plt.axis('off')\n",
    "\n",
    "# go over columns (images)\n",
    "for image_idx in range(len(idx_list)):\n",
    "    # Targets first\n",
    "    result_m = method_results_subset[0][image_idx]\n",
    "    image_fishy = result_m[f\"image_{mymode}\"]\n",
    "    ax = axes[0, 3*image_idx]\n",
    "    ax.imshow(image_fishy)\n",
    "    if image_idx == 0:\n",
    "        ax.annotate(\"Target\\n\", xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center', rotation=90)\n",
    "    ax.annotate(f\" \\n\\nReconstruction\", xy=(0.5, 1), xytext=(0, 40),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    target_seg_fishy = result_m[f\"target_seg_{mymode}\"]\n",
    "    ax = axes[0, 3*image_idx+1]\n",
    "    ax.imshow(target_seg_fishy)\n",
    "    ax.annotate(f\"{letters[image_idx]}\\n\\nClass/Diff Map\", xy=(0.5, 1), xytext=(0, 40),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    target_seg_fishy = result_m[f\"target_detect_{mymode}\"]\n",
    "    ax = axes[0, 3*image_idx+2]\n",
    "    ax.imshow(target_seg_fishy)\n",
    "    ax.annotate(f\" \\n\\nLocalization\", xy=(0.5, 1), xytext=(0, 40),\n",
    "                xycoords='axes fraction', textcoords='offset points',\n",
    "                size=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    \n",
    "\n",
    "    # go over rows (methods)\n",
    "    for method_idx in range(len(method_results_subset)):\n",
    "        result_m = method_results_subset[method_idx][image_idx]\n",
    "        pred_seg_fishy = result_m[f\"ae_{mymode}\"]\n",
    "        ax = axes[method_idx+1, 3*image_idx]\n",
    "        ax.imshow(pred_seg_fishy)\n",
    "        if image_idx == 0:\n",
    "            ax.annotate(method_list_subset[method_idx][\"column1\"], xy=(-0.1, 0.5), xytext=(-20, 0),\n",
    "                xycoords='axes fraction', textcoords='offset points', rotation=90,\n",
    "                size=14, ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "        pred_seg_fishy = result_m[f\"pred_seg_{mymode}\"]\n",
    "        ax = axes[method_idx+1, 3*image_idx+1]\n",
    "        ax.imshow(pred_seg_fishy)\n",
    "        ax.axis('off')\n",
    "        pred_seg_fishy = result_m[f\"pred_detect_{mymode}\"]\n",
    "        ax = axes[method_idx+1, 3*image_idx+2]\n",
    "        ax.imshow(pred_seg_fishy)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.savefig(f\"./evaluations/results_final_bad_{mymode}_1.pdf\", bbox_inches='tight')\n",
    "#plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patch_classification",
   "language": "python",
   "name": "patch_classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}